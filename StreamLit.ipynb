{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StreamLit",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUpZnNxCaEVu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "!pip install transformers\n",
        "import transformers\n",
        "!pip install streamlit\n",
        "!pip install pyngrok==4.2.2\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Kj27sPZEaB",
        "outputId": "70622505-ce1f-479c-f454-3e5ca3360889",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lksN1U6yxI1",
        "outputId": "416671fe-92af-4ea4-a87b-29cfdd583955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "PAGE_CONFIG = {\"page_title\": \"AI Answer Evaluation\", \"page_icon\": \"ðŸ“š\",\"layout\": \"centered\"}\n",
        "st.set_page_config(**PAGE_CONFIG)\n",
        "\n",
        "import streamlit as st\n",
        "import streamlit as st\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        " \n",
        " \n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "<style>\n",
        ".sidebar .sidebar-content {\n",
        "    background-image: linear-gradient(rgb(54,112,130), rgb(54,112,130));\n",
        "    color: white;\n",
        "    size = 30px ; \n",
        "    style: Times New Roman;\n",
        "}\n",
        "</style>\n",
        "\"\"\",\n",
        "    unsafe_allow_html=True,)\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "<style>\n",
        ".sidebar .selectbox .selectbox-content {\n",
        "    background-image: linear-gradient(rgb(54,112,130), rgb(54,112,130));\n",
        "    color: white;\n",
        "    size: 30px ; \n",
        "    style: Times New Roman;\n",
        "}\n",
        "</style>\n",
        "\"\"\",\n",
        "    unsafe_allow_html=True,)\n",
        "\n",
        "#Input\n",
        "m_c_m = \"\"\"<div style= padding: 10px;\"><h1 style = \"color: rgb(199,147,74 ); text-align: center; style = Times New Roman; font-size: 60x;\">AI based Answer Evaluation</h1></div>\"\"\"\n",
        "\n",
        " \n",
        "st.markdown(m_c_m, unsafe_allow_html=True)\n",
        "\n",
        "colouringTool = '''\n",
        "<style>\n",
        ".stNumberInput>div>div>input {\n",
        "    color: #4F8BF9;\n",
        "}\n",
        "</style>\n",
        "'''\n",
        "\n",
        "st.markdown(colouringTool,unsafe_allow_html = True)\n",
        "\n",
        "\n",
        "page_bg_img = '''\n",
        "<style>\n",
        "body {\n",
        "background-image: url(\"https://papers.co/wallpaper/papers.co-vk51-android-lollipop-material-design-dark-yellow-pattern-36-3840x2400-4k-wallpaper.jpg\");\n",
        "background-size: cover;\n",
        "}\n",
        "</style>\n",
        "'''\n",
        "st.markdown(page_bg_img, unsafe_allow_html=True)\n",
        "st.sidebar.text(\"Select Demo Type\")\n",
        "text = 'white'\n",
        "\n",
        "s = st.sidebar.selectbox((\"\"), [\"Demo\", \"Google Forms\"])\n",
        "if s == \"Demo\":\n",
        " \n",
        "  m_c_m = \"\"\"<div style=  padding: 10px;\"><h2 style = \"color: black; text-align: center;\">\"AI based Answer Evaluation\" </h2></div>\"\"\"\n",
        "\n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, \"Standard Answer:\"), unsafe_allow_html=True)\n",
        "  sentence1 = st.text_input(' ')\n",
        "  \n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, \"Students Answer:\"), unsafe_allow_html=True)\n",
        "  sentence2 = st.text_input('   ')\n",
        "\n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text,\"Maximum Marks\"), unsafe_allow_html=True)\n",
        "  marks =  st.number_input('                           ')\n",
        "\n",
        "  stop_words = set(stopwords.words())\n",
        "  words = word_tokenize(sentence2)\n",
        "  sentence3 = []\n",
        "  for w in words:\n",
        "    if w not in stop_words:\n",
        "      sentence3.append(w)\n",
        "\n",
        "  important = []\n",
        "  do_you_want =   st.write('<span style=\"color:%s\">%s</span>' % (text,\"Do you want to use keyword search\"), unsafe_allow_html=True)\n",
        "  want_key = st.selectbox(\" \" , [\"no\",\"yes\",])\n",
        "\n",
        "  points = 0\n",
        "  if want_key == \"yes\":\n",
        "    st.write('<span style=\"color:%s\">%s</span>' % (text,\"Enter the important keywords: \"), unsafe_allow_html=True)\n",
        "    keywords = st.text_input('                                                       ')\n",
        "    words1 = word_tokenize(keywords)\n",
        "\n",
        "    for z in words1:\n",
        "      if z not in sentence3:\n",
        "        points = points + 1\n",
        "\n",
        "    st.write('<span style=\"color:%s\">%s</span>' % (text,\"Enter the marks to be given for not writng the keywords\"), unsafe_allow_html=True)\n",
        "    key_mark = st.number_input(\"       \")\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  from spellchecker import SpellChecker\n",
        "\n",
        "  spell = SpellChecker()\n",
        "  #techer\n",
        "  words = word_tokenize(sentence1)\n",
        "  misspelled = spell.unknown(words)\n",
        "  for i in spell.unknown(words):\n",
        "    st.write('<span style=\"color:%s\">%s</span>' % (' ',\"Did you mean\", spell.correction(i)), unsafe_allow_html=True)\n",
        "     \n",
        "\n",
        "  # find those words that may be misspelled\n",
        "  words = word_tokenize(sentence2)\n",
        "  misspelled = spell.unknown(words)\n",
        "  wrong = 0\n",
        "  \n",
        "  \n",
        "  for word in misspelled:\n",
        "\n",
        "    \n",
        "    wrong = wrong + 1\n",
        "    \n",
        "  new = []\n",
        "  for i in words:\n",
        "    i = spell.correction(i)\n",
        "    new.append(i)\n",
        "  \n",
        "    # Get the one `most likely` answer\n",
        "      \n",
        "  \n",
        "\n",
        "  if st.button(\"Evaluate\"):\n",
        "  \n",
        "        \n",
        "    labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "    max_length = 128  # Maximum length of input sentence to the model.\n",
        "    batch_size = 32\n",
        "    epochs = 2\n",
        " \n",
        " \n",
        "    # Create the model under a distribution strategy scope.\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "    with strategy.scope():\n",
        "        # Encoded token ids from BERT tokenizer.\n",
        "        input_ids = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
        "        )\n",
        "        # Attention masks indicates to the model which tokens should be attended to.\n",
        "        attention_masks = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
        "        )\n",
        "        # Token type ids are binary masks identifying different sequences in the model.\n",
        "        token_type_ids = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
        "        )\n",
        "        # Loading pretrained BERT model.\n",
        "        bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
        "\n",
        "        sequence_output, pooled_output = bert_model(\n",
        "            input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
        "        )\n",
        "        # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
        "        bi_lstm = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(64, return_sequences=True)\n",
        "        )(sequence_output)\n",
        "        # Applying hybrid pooling approach to bi_lstm sequence output.\n",
        "        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
        "        max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
        "        concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "        dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "        output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n",
        "\n",
        "        model = tf.keras.models.Model(\n",
        "            inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
        "        )\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(),\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            metrics=[\"acc\"],\n",
        "        )\n",
        "\n",
        "    model.load_weights(r\"/content/drive/My Drive/Project_NLP.h5\")\n",
        "\n",
        "\n",
        "    class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
        "        \"\"\"Generates batches of data.\n",
        "\n",
        "        Args:\n",
        "            sentence_pairs: Array of premise and hypothesis input sentences.\n",
        "            labels: Array of labels.\n",
        "            batch_size: Integer batch size.\n",
        "            shuffle: boolean, whether to shuffle the data.\n",
        "            include_targets: boolean, whether to incude the labels.\n",
        "\n",
        "        Returns:\n",
        "            Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
        "            (or just `[input_ids, attention_mask, `token_type_ids]`\n",
        "            if `include_targets=False`)\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(\n",
        "                self,\n",
        "                sentence_pairs,\n",
        "                labels,\n",
        "                batch_size=1,\n",
        "                shuffle=True,\n",
        "                include_targets=True,\n",
        "        ):\n",
        "            self.sentence_pairs = sentence_pairs\n",
        "            self.labels = labels\n",
        "            self.shuffle = shuffle\n",
        "            self.batch_size = batch_size\n",
        "            self.include_targets = include_targets\n",
        "            # Load our BERT Tokenizer to encode the text.\n",
        "            # We will use base-base-uncased pretrained model.\n",
        "            self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "                \"bert-base-uncased\", do_lower_case=True\n",
        "            )\n",
        "            self.indexes = np.arange(len(self.sentence_pairs))\n",
        "            self.on_epoch_end()\n",
        "\n",
        "        def __len__(self):\n",
        "            # Denotes the number of batches per epoch.\n",
        "            return len(self.sentence_pairs) // self.batch_size\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            # Retrieves the batch of index.\n",
        "            indexes = self.indexes[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "            sentence_pairs = self.sentence_pairs[indexes]\n",
        "\n",
        "            # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
        "            # encoded together and separated by [SEP] token.\n",
        "            encoded = self.tokenizer.batch_encode_plus(\n",
        "                sentence_pairs.tolist(),\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_length,\n",
        "                return_attention_mask=True,\n",
        "                return_token_type_ids=True,\n",
        "                pad_to_max_length=True,\n",
        "                return_tensors=\"tf\",\n",
        "            )\n",
        "\n",
        "            # Convert batch of encoded features to numpy array.\n",
        "            input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "            attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "            token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "            # Set to true if data generator is used for training/validation.\n",
        "            if self.include_targets:\n",
        "                labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
        "                return [input_ids, attention_masks, token_type_ids], labels\n",
        "            else:\n",
        "                return [input_ids, attention_masks, token_type_ids]\n",
        "\n",
        "        def on_epoch_end(self):\n",
        "            # Shuffle indexes after each epoch if shuffle is set to True.\n",
        "            if self.shuffle:\n",
        "                np.random.RandomState(42).shuffle(self.indexes)\n",
        "\n",
        "\n",
        "    def check_similarity(sentence1, sentence2, marks):\n",
        "        sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
        "        test_data = BertSemanticDataGenerator(\n",
        "            sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
        "            )\n",
        "\n",
        "        proba = model.predict(test_data)[0]\n",
        "        idx = np.argmax(proba)\n",
        "        #proba = f\"{proba[idx]: .2f}%\"\n",
        "        pred = labels[idx]\n",
        "        return proba[idx]*marks\n",
        "\n",
        "\n",
        "\n",
        "    points1 = check_similarity(sentence1,new,marks)\n",
        "    \n",
        "    points1 = round(points1)\n",
        "    if int(wrong) == 0:\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % ('Chartreuse', \"There are 0 spelling mistakes\"), unsafe_allow_html=True)\n",
        "       \n",
        "      \n",
        "    if int(wrong) == 1:\n",
        "      s = (\"There is 1 spelling mistake\")\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % ('red', s), unsafe_allow_html=True)\n",
        "\n",
        "    if int(wrong)>1:\n",
        "      s  = (\"There are \", wrong , \"spelling mistakes\")\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % ('red', s), unsafe_allow_html=True)\n",
        "\n",
        "    if want_key == \"yes\":\n",
        "      final = points1 - ((wrong)*25/100) - (points*key_mark)\n",
        "    \n",
        "    if want_key==\"no\":\n",
        "      final = points1 - ((wrong)*25/100)\n",
        "\n",
        "\n",
        "    if points1<(marks/2):\n",
        "      s = (\"Not much correct you got\", final)\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % ('red', s), unsafe_allow_html=True)\n",
        "    else:\n",
        "      s = (f\"Good\", final)\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % ('Chartreuse', s), unsafe_allow_html=True)\n",
        "\n",
        "     \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "if s==\"Google Forms\":\n",
        "   \n",
        "\n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, 'The data is read from the google form and diplayed in a google sheet'), unsafe_allow_html=True)  \n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, 'This is the link of the google form :-'), unsafe_allow_html=True)  \n",
        "  st.write(\"https://forms.gle/jQqYUgXuN48MX1tv8\")\n",
        "\n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, 'Enter the number of questions in the google form'), unsafe_allow_html=True)  \n",
        "  number_of_questions = st.number_input(\"           \")\n",
        "\n",
        "  \n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, 'Enter the maximum marks'), unsafe_allow_html=True)  \n",
        "  marks = st.number_input(\"                                                                           \")\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "  import gspread\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "\n",
        "  gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "  worksheet = gc.open('Responses').sheet1\n",
        "  rows = worksheet.get_all_values()\n",
        "  worksheet = gc.open('Responses').sheet1\n",
        "  # Convert to a DataFrame and render.\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame.from_records(rows)\n",
        "  \n",
        "  st.table(df)\n",
        "  \n",
        "  if st.button(\"Evaluate Google Forms\"):\n",
        "    \n",
        "   # Get the one `most likely` answer\n",
        "        \n",
        "    points = 0\n",
        "\n",
        "  \n",
        "    labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "\n",
        "    max_length = 128  # Maximum length of input sentence to the model.\n",
        "    batch_size = 32\n",
        "    epochs = 2\n",
        "\n",
        "    # Labels in our dataset.\n",
        "    labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "\n",
        "    # Create the model under a distribution strategy scope.\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "    with strategy.scope():\n",
        "        # Encoded token ids from BERT tokenizer.\n",
        "        input_ids = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
        "        )\n",
        "        # Attention masks indicates to the model which tokens should be attended to.\n",
        "        attention_masks = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
        "        )\n",
        "        # Token type ids are binary masks identifying different sequences in the model.\n",
        "        token_type_ids = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
        "        )\n",
        "        # Loading pretrained BERT model.\n",
        "        bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
        "\n",
        "        sequence_output, pooled_output = bert_model(\n",
        "            input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
        "        )\n",
        "        # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
        "        bi_lstm = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(64, return_sequences=True)\n",
        "        )(sequence_output)\n",
        "        # Applying hybrid pooling approach to bi_lstm sequence output.\n",
        "        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
        "        max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
        "        concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "        dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "        output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n",
        "\n",
        "        model = tf.keras.models.Model(\n",
        "            inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
        "        )\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(),\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            metrics=[\"acc\"],\n",
        "        )\n",
        "\n",
        "    model.load_weights(r\"/content/drive/My Drive/Project_NLP.h5\")\n",
        "\n",
        "\n",
        "    class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
        "        \"\"\"Generates batches of data.\n",
        "\n",
        "        Args:\n",
        "            sentence_pairs: Array of premise and hypothesis input sentences.\n",
        "            labels: Array of labels.\n",
        "            batch_size: Integer batch size.\n",
        "            shuffle: boolean, whether to shuffle the data.\n",
        "            include_targets: boolean, whether to incude the labels.\n",
        "\n",
        "        Returns:\n",
        "            Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
        "            (or just `[input_ids, attention_mask, `token_type_ids]`\n",
        "            if `include_targets=False`)\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(\n",
        "                self,\n",
        "                sentence_pairs,\n",
        "                labels,\n",
        "                batch_size=1,\n",
        "                shuffle=True,\n",
        "                include_targets=True,\n",
        "        ):\n",
        "            self.sentence_pairs = sentence_pairs\n",
        "            self.labels = labels\n",
        "            self.shuffle = shuffle\n",
        "            self.batch_size = batch_size\n",
        "            self.include_targets = include_targets\n",
        "            # Load our BERT Tokenizer to encode the text.\n",
        "            # We will use base-base-uncased pretrained model.\n",
        "            self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "                \"bert-base-uncased\", do_lower_case=True\n",
        "            )\n",
        "            self.indexes = np.arange(len(self.sentence_pairs))\n",
        "            self.on_epoch_end()\n",
        "\n",
        "        def __len__(self):\n",
        "            # Denotes the number of batches per epoch.\n",
        "            return len(self.sentence_pairs) // self.batch_size\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            # Retrieves the batch of index.\n",
        "            indexes = self.indexes[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "            sentence_pairs = self.sentence_pairs[indexes]\n",
        "\n",
        "            # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
        "            # encoded together and separated by [SEP] token.\n",
        "            encoded = self.tokenizer.batch_encode_plus(\n",
        "                sentence_pairs.tolist(),\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_length,\n",
        "                return_attention_mask=True,\n",
        "                return_token_type_ids=True,\n",
        "                pad_to_max_length=True,\n",
        "                return_tensors=\"tf\",\n",
        "            )\n",
        "\n",
        "            # Convert batch of encoded features to numpy array.\n",
        "            input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "            attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "            token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "            # Set to true if data generator is used for training/validation.\n",
        "            if self.include_targets:\n",
        "                labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
        "                return [input_ids, attention_masks, token_type_ids], labels\n",
        "            else:\n",
        "                return [input_ids, attention_masks, token_type_ids]\n",
        "\n",
        "        def on_epoch_end(self):\n",
        "            # Shuffle indexes after each epoch if shuffle is set to True.\n",
        "            if self.shuffle:\n",
        "                np.random.RandomState(42).shuffle(self.indexes)\n",
        "      \n",
        "        \n",
        "\n",
        "\n",
        "    def check_similarity(sentence1, sentence2, marks):\n",
        "        sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
        "        test_data = BertSemanticDataGenerator(\n",
        "            sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
        "            )\n",
        "    \n",
        "\n",
        "        proba = model.predict(test_data)[0]\n",
        "        idx = np.argmax(proba)\n",
        "        #proba = f\"{proba[idx]: .2f}%\"\n",
        "        pred = labels[idx]\n",
        "        return proba[idx]*marks\n",
        "\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    import gspread\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "\n",
        "    gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "    worksheet = gc.open('Responses').sheet1\n",
        "\n",
        "    # get_all_values gives a list of rows.\n",
        "  \n",
        " \n",
        "    num_of_rows = number_of_questions+1\n",
        "    l = []\n",
        "    h = 1\n",
        "    while h <= num_of_rows:\n",
        "      h = h + 1\n",
        "      cols = worksheet.col_values(h)\n",
        "\n",
        "      for j in cols: \n",
        "        l.append(j)\n",
        "      sentence1 = l[1]   \n",
        "      st.write('<span style=\"color:%s\">%s</span>' % (text, \"Standard Answer\"), unsafe_allow_html=True)\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % (text, sentence1), unsafe_allow_html=True)\n",
        "\n",
        "       \n",
        "      \n",
        "      for x in l:\n",
        "        sentence2 = x\n",
        "        \n",
        "      \n",
        "        stop_words = set(stopwords.words())\n",
        "        words = word_tokenize(sentence2)\n",
        "        sentence3 = []\n",
        "        for w in words:\n",
        "          if w not in stop_words:\n",
        "            sentence3.append(w)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      from spellchecker import SpellChecker\n",
        "\n",
        "      spell = SpellChecker()\n",
        "      #techer\n",
        "      words = word_tokenize(sentence1)\n",
        "      misspelled = spell.unknown(words)\n",
        "      i = 0\n",
        "      for i in spell.unknown(words):\n",
        "        s = (\"Did you mean\", spell.correction(i))\n",
        "        st.write('<span style=\"color:%s\">%s</span>' % (text, s), unsafe_allow_html=True)\n",
        "        \n",
        "\n",
        "      # find those words that may be misspelled\n",
        "      words = word_tokenize(sentence2)\n",
        "      misspelled = spell.unknown(words)\n",
        "      wrong = 0\n",
        "      \n",
        "      \n",
        "      for word in misspelled:\n",
        "\n",
        "        \n",
        "        wrong = wrong + 1\n",
        "        \n",
        "      new = []\n",
        "      for i in words:\n",
        "        i = spell.correction(i)\n",
        "        new.append(i)\n",
        "      \n",
        "        \n",
        "\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % (text, \"Student's Answer\"), unsafe_allow_html=True)\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % (text, sentence2), unsafe_allow_html=True)\n",
        " \n",
        " \n",
        "      points1 = check_similarity(sentence1,new,marks)\n",
        "      \n",
        "      points1 = round(points1)\n",
        "\n",
        "      if int(wrong) == 1:\n",
        "        s = (\"There is one spelling mistake\")\n",
        "        st.write('<span style=\"color:%s\">%s</span>' % (text,s), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "      else:\n",
        "        s = (\"There is \", wrong, \"spelling mistake\")\n",
        "        st.write('<span style=\"color:%s\">%s</span>' % (text,s), unsafe_allow_html=True)\n",
        "      \n",
        "\n",
        "      final = points1 - ((wrong)*25/100)\n",
        "      if points1<(marks/2):\n",
        "        s  = (\"Not much correct\",   (final))\n",
        "        st.write('<span style=\"color:%s\">%s</span>' % (text,s), unsafe_allow_html=True)\n",
        "         \n",
        "      else:\n",
        "        s = (\"Good\",  (final))\n",
        "        st.write('<span style=\"color:%s\">%s</span>' % (text, s), unsafe_allow_html=True)\n",
        "         \n",
        "         \n",
        "\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hnIsNqGOdiX",
        "outputId": "53ed259c-f20b-401b-c51e-f628dde7ebdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  drive  main.py  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om67tRisPnX4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEK_e4ercT32",
        "outputId": "448071d0-cead-483c-e936-24ed2157c0f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ngrok authentoken 1j62rNg2kfTVfaIfaCcUFAWFC0u_2Qbv5ViT93d8NQX6nNF3p"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME:\n",
            "   ngrok - tunnel local ports to public URLs and inspect traffic\n",
            "\n",
            "DESCRIPTION:\n",
            "    ngrok exposes local networked services behinds NATs and firewalls to the\n",
            "    public internet over a secure tunnel. Share local websites, build/test\n",
            "    webhook consumers and self-host personal services.\n",
            "    Detailed help for each command is available with 'ngrok help <command>'.\n",
            "    Open http://localhost:4040 for ngrok's web interface to inspect traffic.\n",
            "\n",
            "EXAMPLES:\n",
            "    ngrok http 80                    # secure public URL for port 80 web server\n",
            "    ngrok http -subdomain=baz 8080   # port 8080 available at baz.ngrok.io\n",
            "    ngrok http foo.dev:80            # tunnel to host:port instead of localhost\n",
            "    ngrok http https://localhost     # expose a local https server\n",
            "    ngrok tcp 22                     # tunnel arbitrary TCP traffic to port 22\n",
            "    ngrok tls -hostname=foo.com 443  # TLS traffic for foo.com to port 443\n",
            "    ngrok start foo bar baz          # start tunnels from the configuration file\n",
            "\n",
            "VERSION:\n",
            "   2.3.35\n",
            "\n",
            "AUTHOR:\n",
            "  inconshreveable - <alan@ngrok.com>\n",
            "\n",
            "COMMANDS:\n",
            "   authtoken\tsave authtoken to configuration file\n",
            "   credits\tprints author and licensing information\n",
            "   http\t\tstart an HTTP tunnel\n",
            "   start\tstart tunnels by name from the configuration file\n",
            "   tcp\t\tstart a TCP tunnel\n",
            "   tls\t\tstart a TLS tunnel\n",
            "   update\tupdate ngrok to the latest version\n",
            "   version\tprint the version string\n",
            "   help\t\tShows a list of commands or help for one command\n",
            "\n",
            "ERROR:  Unrecognized command: authentoken\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnjQc0DeMjdI",
        "outputId": "ebd4ed5d-7624-49e5-fcf1-7581c05cc686",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!streamlit run main.py&>/dev/null&\n",
        "!pgrep streamlit\n",
        " "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY9rmN4aQCIp"
      },
      "source": [
        "localhost:8501"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1RB7fud-Onb",
        "outputId": "c456d382-0e8d-44c1-f933-9723aec87283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(port='8501')\n",
        "public_url      "
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Support for \"return_ngrok_tunnel\" as \"False\" is deprecated and will be removed in 5.0.0, when this method will return a NgrokTunnel instead of a str\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://0a284e47acdd.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MPb_bomRU9Y"
      },
      "source": [
        "!streamlit run main.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lC0Fpnwb4E2",
        "outputId": "59891400-4606-4631-99ba-b7fc63d2917c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "worksheet = gc.open('Responses').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        " \n",
        "\n",
        "number_of_questions = int(input(\"Enter the number of questions: \"))\n",
        "num_of_rows = number_of_questions+1\n",
        "l = []\n",
        "i = 2\n",
        "while i <= num_of_rows:\n",
        "  col_value = worksheet.col_values(i)\n",
        "  for y in col_value:\n",
        "    l.append(y)\n",
        "  i = i+1\n",
        "  \n",
        "  z = 0\n",
        " \n",
        " \n",
        "\n",
        "p = int(len(l)/number_of_questions)\n",
        "ques = (l[0:p])\n",
        " \n",
        "wow = (l[p:])\n",
        "wow\n",
        "\n",
        "ques[1]\n",
        "ques[2]\n",
        "\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the number of questions: 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Republic c is when the king rules'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rI4Wn_okB7O"
      },
      "source": [
        "  \n",
        "      points1 = check_similarity(sentence1,new,marks)\n",
        "      \n",
        "      points1 = round(points1)\n",
        "    \n",
        "      if int(wrong) == 1:\n",
        "        st.write(\"There is \", wrong, \"spelling mistake\")\n",
        "      else:\n",
        "        st.write(\"There are \" , wrong , \"spelling mistakes\")\n",
        "\n",
        "      \n",
        "      final = points1 - ((wrong)*25/100)\n",
        "\n",
        "\n",
        "      if points1<(marks/2):\n",
        "        st.write(\"Not much correct\",   (final))\n",
        "      if points1>(marks/2):\n",
        "        st.write(\"Good\",  (final))\n",
        "\n",
        "      \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIqRqX22tKmL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}