{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StreamLit",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUpZnNxCaEVu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "!pip install transformers\n",
        "import transformers\n",
        "!pip install streamlit\n",
        "!pip install pyngrok==4.2.2\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Kj27sPZEaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c19e5b1-874c-4c1f-9af1-f0deddbbab73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lksN1U6yxI1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9b3d60b-525c-426e-9f93-3984e8c44e63"
      },
      "source": [
        "%%writefile main.py\n",
        "import streamlit as st\n",
        "PAGE_CONFIG = {\"page_title\": \"AI Answer Evaluation\", \"page_icon\": \"ðŸ“š\",\"layout\": \"centered\"}\n",
        "st.set_page_config(**PAGE_CONFIG)\n",
        "\n",
        "import streamlit as st\n",
        "import streamlit as st\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        " \n",
        " \n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "<style>\n",
        ".sidebar .sidebar-content {\n",
        "    background-image: linear-gradient(rgb(54,112,130), rgb(54,112,130));\n",
        "    color: white;\n",
        "    size = 30px ; \n",
        "    style: Times New Roman;\n",
        "}\n",
        "</style>\n",
        "\"\"\",\n",
        "    unsafe_allow_html=True,)\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "<style>\n",
        ".sidebar .selectbox .selectbox-content {\n",
        "    background-image: linear-gradient(rgb(54,112,130), rgb(54,112,130));\n",
        "    color: white;\n",
        "    size: 30px ; \n",
        "    style: Times New Roman;\n",
        "}\n",
        "</style>\n",
        "\"\"\",\n",
        "    unsafe_allow_html=True,)\n",
        "\n",
        "#Input\n",
        "m_c_m = \"\"\"<div style= padding: 10px;\"><h1 style = \"color: rgb(199,147,74 ); text-align: center; style = Times New Roman; font-size: 60x;\">AI based Answer Evaluation</h1></div>\"\"\"\n",
        "\n",
        " \n",
        "st.markdown(m_c_m, unsafe_allow_html=True)\n",
        "\n",
        "colouringTool = '''\n",
        "<style>\n",
        ".stNumberInput>div>div>input {\n",
        "    color: #4F8BF9;\n",
        "}\n",
        "</style>\n",
        "'''\n",
        "\n",
        "st.markdown(colouringTool,unsafe_allow_html = True)\n",
        "\n",
        "\n",
        "page_bg_img = '''\n",
        "<style>\n",
        "body {\n",
        "background-image: url(\"https://papers.co/wallpaper/papers.co-vk51-android-lollipop-material-design-dark-yellow-pattern-36-3840x2400-4k-wallpaper.jpg\");\n",
        "background-size: cover;\n",
        "}\n",
        "</style>\n",
        "'''\n",
        "st.markdown(page_bg_img, unsafe_allow_html=True)\n",
        "st.sidebar.text(\"Select Demo Type\")\n",
        "text = 'white'\n",
        "\n",
        "s = st.sidebar.selectbox((\"\"), [\"Demo\", \"Google Forms\"])\n",
        "if s == \"Demo\":\n",
        " \n",
        "  m_c_m = \"\"\"<div style=  padding: 10px;\"><h2 style = \"color: black; text-align: center;\">\"AI based Answer Evaluation\" </h2></div>\"\"\"\n",
        "\n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, \"Standard Answer:\"), unsafe_allow_html=True)\n",
        "  sentence1 = st.text_input(' ')\n",
        "  \n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, \"Students Answer:\"), unsafe_allow_html=True)\n",
        "  sentence2 = st.text_input('   ')\n",
        "\n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text,\"Maximum Marks\"), unsafe_allow_html=True)\n",
        "  marks =  st.number_input('                           ')\n",
        "\n",
        "  stop_words = set(stopwords.words())\n",
        "  words = word_tokenize(sentence2)\n",
        "  sentence3 = []\n",
        "  for w in words:\n",
        "    if w not in stop_words:\n",
        "      sentence3.append(w)\n",
        "\n",
        "  important = []\n",
        "  do_you_want =   st.write('<span style=\"color:%s\">%s</span>' % (text,\"Do you want to use keyword search\"), unsafe_allow_html=True)\n",
        "  want_key = st.selectbox(\" \" , [\"no\",\"yes\",])\n",
        "\n",
        "  points = 0\n",
        "  if want_key == \"yes\":\n",
        "    st.write('<span style=\"color:%s\">%s</span>' % (text,\"Enter the important keywords: \"), unsafe_allow_html=True)\n",
        "    keywords = st.text_input('                                                       ')\n",
        "    words1 = word_tokenize(keywords)\n",
        "\n",
        "    for z in words1:\n",
        "      if z not in sentence3:\n",
        "        points = points + 1\n",
        "\n",
        "    st.write('<span style=\"color:%s\">%s</span>' % (text,\"Enter the marks to be given for not writng the keywords\"), unsafe_allow_html=True)\n",
        "    key_mark = st.number_input(\"       \")\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  from spellchecker import SpellChecker\n",
        "\n",
        "  spell = SpellChecker()\n",
        "  #techer\n",
        "  words = word_tokenize(sentence1)\n",
        "  misspelled = spell.unknown(words)\n",
        "  for i in spell.unknown(words):\n",
        "    s = (\"Did you mean\", spell.correction(i))\n",
        "    st.write('<span style=\"color:%s\">%s</span>' % (text ,s ), unsafe_allow_html=True)\n",
        "     \n",
        "\n",
        "  # find those words that may be misspelled\n",
        "  words = word_tokenize(sentence2)\n",
        "  misspelled = spell.unknown(words)\n",
        "  wrong = 0\n",
        "  \n",
        "  \n",
        "  for word in misspelled:\n",
        "\n",
        "    \n",
        "    wrong = wrong + 1\n",
        "    \n",
        "  new = []\n",
        "  for i in words:\n",
        "    i = spell.correction(i)\n",
        "    new.append(i)\n",
        "  \n",
        "    # Get the one `most likely` answer\n",
        "      \n",
        "  \n",
        "\n",
        "  if st.button(\"Evaluate\"):\n",
        "  \n",
        "        \n",
        "    labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "    max_length = 128  # Maximum length of input sentence to the model.\n",
        "    batch_size = 32\n",
        "    epochs = 2\n",
        " \n",
        " \n",
        "    # Create the model under a distribution strategy scope.\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "    with strategy.scope():\n",
        "        # Encoded token ids from BERT tokenizer.\n",
        "        input_ids = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
        "        )\n",
        "        # Attention masks indicates to the model which tokens should be attended to.\n",
        "        attention_masks = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
        "        )\n",
        "        # Token type ids are binary masks identifying different sequences in the model.\n",
        "        token_type_ids = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
        "        )\n",
        "        # Loading pretrained BERT model.\n",
        "        bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
        "\n",
        "        sequence_output, pooled_output = bert_model(\n",
        "            input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
        "        )\n",
        "        # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
        "        bi_lstm = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(64, return_sequences=True)\n",
        "        )(sequence_output)\n",
        "        # Applying hybrid pooling approach to bi_lstm sequence output.\n",
        "        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
        "        max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
        "        concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "        dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "        output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n",
        "\n",
        "        model = tf.keras.models.Model(\n",
        "            inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
        "        )\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(),\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            metrics=[\"acc\"],\n",
        "        )\n",
        "\n",
        "    model.load_weights(r\"/content/drive/My Drive/Project_NLP.h5\")\n",
        "\n",
        "\n",
        "    class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
        "        \"\"\"Generates batches of data.\n",
        "\n",
        "        Args:\n",
        "            sentence_pairs: Array of premise and hypothesis input sentences.\n",
        "            labels: Array of labels.\n",
        "            batch_size: Integer batch size.\n",
        "            shuffle: boolean, whether to shuffle the data.\n",
        "            include_targets: boolean, whether to incude the labels.\n",
        "\n",
        "        Returns:\n",
        "            Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
        "            (or just `[input_ids, attention_mask, `token_type_ids]`\n",
        "            if `include_targets=False`)\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(\n",
        "                self,\n",
        "                sentence_pairs,\n",
        "                labels,\n",
        "                batch_size=1,\n",
        "                shuffle=True,\n",
        "                include_targets=True,\n",
        "        ):\n",
        "            self.sentence_pairs = sentence_pairs\n",
        "            self.labels = labels\n",
        "            self.shuffle = shuffle\n",
        "            self.batch_size = batch_size\n",
        "            self.include_targets = include_targets\n",
        "            # Load our BERT Tokenizer to encode the text.\n",
        "            # We will use base-base-uncased pretrained model.\n",
        "            self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "                \"bert-base-uncased\", do_lower_case=True\n",
        "            )\n",
        "            self.indexes = np.arange(len(self.sentence_pairs))\n",
        "            self.on_epoch_end()\n",
        "\n",
        "        def __len__(self):\n",
        "            # Denotes the number of batches per epoch.\n",
        "            return len(self.sentence_pairs) // self.batch_size\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            # Retrieves the batch of index.\n",
        "            indexes = self.indexes[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "            sentence_pairs = self.sentence_pairs[indexes]\n",
        "\n",
        "            # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
        "            # encoded together and separated by [SEP] token.\n",
        "            encoded = self.tokenizer.batch_encode_plus(\n",
        "                sentence_pairs.tolist(),\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_length,\n",
        "                return_attention_mask=True,\n",
        "                return_token_type_ids=True,\n",
        "                pad_to_max_length=True,\n",
        "                return_tensors=\"tf\",\n",
        "            )\n",
        "\n",
        "            # Convert batch of encoded features to numpy array.\n",
        "            input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "            attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "            token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "            # Set to true if data generator is used for training/validation.\n",
        "            if self.include_targets:\n",
        "                labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
        "                return [input_ids, attention_masks, token_type_ids], labels\n",
        "            else:\n",
        "                return [input_ids, attention_masks, token_type_ids]\n",
        "\n",
        "        def on_epoch_end(self):\n",
        "            # Shuffle indexes after each epoch if shuffle is set to True.\n",
        "            if self.shuffle:\n",
        "                np.random.RandomState(42).shuffle(self.indexes)\n",
        "\n",
        "\n",
        "    def check_similarity(sentence1, sentence2, marks):\n",
        "        sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
        "        test_data = BertSemanticDataGenerator(\n",
        "            sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
        "            )\n",
        "\n",
        "        proba = model.predict(test_data)[0]\n",
        "        idx = np.argmax(proba)\n",
        "        #proba = f\"{proba[idx]: .2f}%\"\n",
        "        pred = labels[idx]\n",
        "        return proba[idx]*marks\n",
        "\n",
        "\n",
        "\n",
        "    points1 = check_similarity(sentence1,new,marks)\n",
        "    \n",
        "    points1 = round(points1)\n",
        "    if int(wrong) == 0:\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % ('Chartreuse', \"There are 0 spelling mistakes\"), unsafe_allow_html=True)\n",
        "       \n",
        "      \n",
        "    if int(wrong) == 1:\n",
        "      s = (\"There is 1 spelling mistake\")\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % ('red', s), unsafe_allow_html=True)\n",
        "\n",
        "    if int(wrong)>1:\n",
        "      s  = (\"There are \", wrong , \"spelling mistakes\")\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % ('red', s), unsafe_allow_html=True)\n",
        "\n",
        "    if want_key == \"yes\":\n",
        "      final = points1 - ((wrong)*25/100) - (points*key_mark)\n",
        "    \n",
        "    if want_key==\"no\":\n",
        "      final = points1 - ((wrong)*25/100)\n",
        "\n",
        "\n",
        "    if points1<(marks/2):\n",
        "      s = (\"Not much correct you got\", final)\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % ('red', s), unsafe_allow_html=True)\n",
        "    else:\n",
        "      s = (f\"Good\", final)\n",
        "      st.write('<span style=\"color:%s\">%s</span>' % ('Chartreuse', s), unsafe_allow_html=True)\n",
        "\n",
        "     \n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "if s==\"Google Forms\":\n",
        "   \n",
        "\n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, 'The data is read from the google form and diplayed in a google sheet'), unsafe_allow_html=True)  \n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, 'This is the link of the google form :-'), unsafe_allow_html=True)  \n",
        "  st.write(\"https://forms.gle/jQqYUgXuN48MX1tv8\")\n",
        "\n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, 'Enter the number of questions in the google form'), unsafe_allow_html=True)  \n",
        "  number_of_questions = st.number_input(\"           \")\n",
        "\n",
        "  \n",
        "  st.write('<span style=\"color:%s\">%s</span>' % (text, 'Enter the maximum marks'), unsafe_allow_html=True)  \n",
        "  marks = st.number_input(\"                                                                           \")\n",
        " \n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "  import gspread\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "\n",
        "  gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "  worksheet = gc.open('Responses').sheet1\n",
        "  rows = worksheet.get_all_values()\n",
        "  worksheet = gc.open('Responses').sheet1\n",
        "  # Convert to a DataFrame and render.\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame.from_records(rows)\n",
        "  \n",
        " \n",
        "  \n",
        "  if st.button(\"Evaluate Google Forms\"):\n",
        "    \n",
        "   # Get the one `most likely` answer\n",
        "        \n",
        "    points = 0\n",
        "\n",
        "  \n",
        "    labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "\n",
        "    max_length = 128  # Maximum length of input sentence to the model.\n",
        "    batch_size = 32\n",
        "    epochs = 2\n",
        "\n",
        "    # Labels in our dataset.\n",
        "    labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
        "\n",
        "    # Create the model under a distribution strategy scope.\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "    with strategy.scope():\n",
        "        # Encoded token ids from BERT tokenizer.\n",
        "        input_ids = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"input_ids\"\n",
        "        )\n",
        "        # Attention masks indicates to the model which tokens should be attended to.\n",
        "        attention_masks = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"attention_masks\"\n",
        "        )\n",
        "        # Token type ids are binary masks identifying different sequences in the model.\n",
        "        token_type_ids = tf.keras.layers.Input(\n",
        "            shape=(max_length,), dtype=tf.int32, name=\"token_type_ids\"\n",
        "        )\n",
        "        # Loading pretrained BERT model.\n",
        "        bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        # Freeze the BERT model to reuse the pretrained features without modifying them.\n",
        "\n",
        "        sequence_output, pooled_output = bert_model(\n",
        "            input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
        "        )\n",
        "        # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
        "        bi_lstm = tf.keras.layers.Bidirectional(\n",
        "            tf.keras.layers.LSTM(64, return_sequences=True)\n",
        "        )(sequence_output)\n",
        "        # Applying hybrid pooling approach to bi_lstm sequence output.\n",
        "        avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
        "        max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
        "        concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
        "        dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
        "        output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n",
        "\n",
        "        model = tf.keras.models.Model(\n",
        "            inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
        "        )\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(),\n",
        "            loss=\"categorical_crossentropy\",\n",
        "            metrics=[\"acc\"],\n",
        "        )\n",
        "\n",
        "    model.load_weights(r\"/content/drive/My Drive/Project_NLP.h5\")\n",
        "\n",
        "\n",
        "    class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
        "        \"\"\"Generates batches of data.\n",
        "\n",
        "        Args:\n",
        "            sentence_pairs: Array of premise and hypothesis input sentences.\n",
        "            labels: Array of labels.\n",
        "            batch_size: Integer batch size.\n",
        "            shuffle: boolean, whether to shuffle the data.\n",
        "            include_targets: boolean, whether to incude the labels.\n",
        "\n",
        "        Returns:\n",
        "            Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
        "            (or just `[input_ids, attention_mask, `token_type_ids]`\n",
        "            if `include_targets=False`)\n",
        "        \"\"\"\n",
        "\n",
        "        def __init__(\n",
        "                self,\n",
        "                sentence_pairs,\n",
        "                labels,\n",
        "                batch_size=1,\n",
        "                shuffle=True,\n",
        "                include_targets=True,\n",
        "        ):\n",
        "            self.sentence_pairs = sentence_pairs\n",
        "            self.labels = labels\n",
        "            self.shuffle = shuffle\n",
        "            self.batch_size = batch_size\n",
        "            self.include_targets = include_targets\n",
        "            # Load our BERT Tokenizer to encode the text.\n",
        "            # We will use base-base-uncased pretrained model.\n",
        "            self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
        "                \"bert-base-uncased\", do_lower_case=True\n",
        "            )\n",
        "            self.indexes = np.arange(len(self.sentence_pairs))\n",
        "            self.on_epoch_end()\n",
        "\n",
        "        def __len__(self):\n",
        "            # Denotes the number of batches per epoch.\n",
        "            return len(self.sentence_pairs) // self.batch_size\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            # Retrieves the batch of index.\n",
        "            indexes = self.indexes[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
        "            sentence_pairs = self.sentence_pairs[indexes]\n",
        "\n",
        "            # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
        "            # encoded together and separated by [SEP] token.\n",
        "            encoded = self.tokenizer.batch_encode_plus(\n",
        "                sentence_pairs.tolist(),\n",
        "                add_special_tokens=True,\n",
        "                max_length=max_length,\n",
        "                return_attention_mask=True,\n",
        "                return_token_type_ids=True,\n",
        "                pad_to_max_length=True,\n",
        "                return_tensors=\"tf\",\n",
        "            )\n",
        "\n",
        "            # Convert batch of encoded features to numpy array.\n",
        "            input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
        "            attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
        "            token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
        "\n",
        "            # Set to true if data generator is used for training/validation.\n",
        "            if self.include_targets:\n",
        "                labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
        "                return [input_ids, attention_masks, token_type_ids], labels\n",
        "            else:\n",
        "                return [input_ids, attention_masks, token_type_ids]\n",
        "\n",
        "        def on_epoch_end(self):\n",
        "            # Shuffle indexes after each epoch if shuffle is set to True.\n",
        "            if self.shuffle:\n",
        "                np.random.RandomState(42).shuffle(self.indexes)\n",
        "      \n",
        "        \n",
        "\n",
        "\n",
        "    def check_similarity(sentence1, sentence2, marks):\n",
        "        sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
        "        test_data = BertSemanticDataGenerator(\n",
        "            sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
        "            )\n",
        "    \n",
        "\n",
        "        proba = model.predict(test_data)[0]\n",
        "        idx = np.argmax(proba)\n",
        "        #proba = f\"{proba[idx]: .2f}%\"\n",
        "        pred = labels[idx]\n",
        "        return proba[idx]*marks\n",
        "\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    import gspread\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "\n",
        "    gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "    worksheet = gc.open('Responses').sheet1\n",
        "    data = worksheet.get_all_records()\n",
        "    num_of_rows = len(data)\n",
        "    \n",
        "\n",
        "  \n",
        "    num_of_cols = number_of_questions+1\n",
        "    l = []\n",
        "    h = 1\n",
        "    avg_val=0\n",
        "    avg_marks = []\n",
        "   \n",
        "    \n",
        "    while h < num_of_cols:\n",
        "      h = h + 1\n",
        "\n",
        "      cols = worksheet.col_values(h)\n",
        "\n",
        "      for j in cols: \n",
        "        l.append(j)\n",
        "        #This is sentence 1\n",
        "      sentence1 = l[1]   \n",
        "     \n",
        "      p = 2\n",
        "\n",
        "      while p <= num_of_rows:\n",
        "        p = p+1\n",
        "        sentence2 = worksheet.cell(p,h).value\n",
        "        \n",
        "      \n",
        "        stop_words = set(stopwords.words())\n",
        "        words = word_tokenize(sentence2)\n",
        "        sentence3 = []\n",
        "        for w in words:\n",
        "          if w not in stop_words:\n",
        "            sentence3.append(w)\n",
        "\n",
        "        from spellchecker import SpellChecker\n",
        "\n",
        "        spell = SpellChecker()\n",
        "        #techer\n",
        "        words = word_tokenize(sentence1)\n",
        "        misspelled = spell.unknown(words)\n",
        "        i = 0\n",
        "        \n",
        "\n",
        "        # find those words that may be misspelled\n",
        "        words = word_tokenize(sentence2)\n",
        "        misspelled = spell.unknown(words)\n",
        "        wrong = 0\n",
        "        \n",
        "        \n",
        "        for word in misspelled:\n",
        "            wrong = wrong + 1\n",
        "          \n",
        "        new = []\n",
        "        for i in words:\n",
        "          i = spell.correction(i)\n",
        "          new.append(i)\n",
        "          \n",
        "        points1 = check_similarity(sentence1,new,marks)\n",
        "        points1 = round(points1)\n",
        "        final = points1 - ((wrong)*25/100)\n",
        "     \n",
        "        avg_val = avg_val + (final/marks*100)\n",
        "      avg = (avg_val/num_of_rows)\n",
        "      avg_marks.append(avg)\n",
        "      avg=0\n",
        " \n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([0,0,1,1])\n",
        "    langs = ['Question1', 'Question2']\n",
        "    ax.bar(langs,avg_marks)\n",
        "    st.pyplot(fig)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om67tRisPnX4"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY9rmN4aQCIp"
      },
      "source": [
        "localhost:8501"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1RB7fud-Onb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "545d4d29-9b68-43ee-92f5-ec5339fa1b07"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(port='8501')\n",
        "public_url      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Support for \"return_ngrok_tunnel\" as \"False\" is deprecated and will be removed in 5.0.0, when this method will return a NgrokTunnel instead of a str\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://e7b70777d6a3.ngrok.io'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YjlGg9ZPUWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a51c0349-4999-4c7a-ced8-a752b6ecaa72"
      },
      "source": [
        "!streamlit run main.py                   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.245.91.154:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2020-11-21 12:00:53.845431: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-21 12:05:25.931075: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
            "2020-11-21 12:05:25.931608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-21 12:05:25.932258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-21 12:05:25.932330: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-21 12:05:26.176283: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-21 12:05:26.292186: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-21 12:05:26.336848: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-21 12:05:26.581025: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-21 12:05:26.635292: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-21 12:05:27.130072: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-21 12:05:27.130328: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-21 12:05:27.131058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-21 12:05:27.131621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-21 12:05:27.143575: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2200000000 Hz\n",
            "2020-11-21 12:05:27.143768: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xaaa7b80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-21 12:05:27.143794: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-11-21 12:05:27.281955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-21 12:05:27.282694: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xaaa79c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-11-21 12:05:27.282727: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-11-21 12:05:27.282945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-21 12:05:27.283500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.73GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2020-11-21 12:05:27.283581: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-21 12:05:27.283622: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "2020-11-21 12:05:27.283653: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
            "2020-11-21 12:05:27.283679: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
            "2020-11-21 12:05:27.283701: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
            "2020-11-21 12:05:27.283728: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
            "2020-11-21 12:05:27.283759: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-21 12:05:27.283845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-21 12:05:27.284425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-21 12:05:27.284939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n",
            "2020-11-21 12:05:27.287879: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-11-21 12:05:31.063668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-11-21 12:05:31.063727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \n",
            "2020-11-21 12:05:31.063742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \n",
            "2020-11-21 12:05:31.067523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-21 12:05:31.068210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-11-21 12:05:31.068761: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-11-21 12:05:31.068808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13962 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2020-11-21 12:05:31.074 Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "2020-11-21 12:05:31.369 Lock 139754948910160 acquired on /root/.cache/torch/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n",
            "Downloading: 100% 433/433 [00:00<00:00, 378kB/s]\n",
            "2020-11-21 12:05:31.451 Lock 139754948910160 released on /root/.cache/torch/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170.lock\n",
            "2020-11-21 12:05:31.533 Lock 139754948911056 acquired on /root/.cache/torch/transformers/775efbdc2152093295bc5824dee96da82a5f3c1f218dfface1b8cef3094bdf8f.c719a806caef7d36ec0185f14b3b5fa727d919f924abe35622b4b7147bfbb8c7.h5.lock\n",
            "Downloading: 100% 536M/536M [00:06<00:00, 81.6MB/s]\n",
            "2020-11-21 12:05:38.221 Lock 139754948911056 released on /root/.cache/torch/transformers/775efbdc2152093295bc5824dee96da82a5f3c1f218dfface1b8cef3094bdf8f.c719a806caef7d36ec0185f14b3b5fa727d919f924abe35622b4b7147bfbb8c7.h5.lock\n",
            "2020-11-21 12:05:38.712596: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "2020-11-21 12:05:52.716 Lock 139753535641136 acquired on /root/.cache/torch/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 3.63MB/s]\n",
            "2020-11-21 12:05:52.864 Lock 139753535641136 released on /root/.cache/torch/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "2020-11-21 12:05:53.087 From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Iterator.get_next_as_optional()` instead.\n",
            "2020-11-21 12:05:57.413404: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-11-21 12:06:02.811 Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUBbIFSvcMfh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}